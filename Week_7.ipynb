{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8R+D8bMeE6CtEJUyE3OtU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HallsofLearningCoding/IAOI_Recap/blob/main/Week_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only have two weeks to go :) I am proud of the effort and the improvement that you all have made thus far. So far we are on schedule, and have completed the following:\n",
        "\n",
        "Week 1 - 2: Python Programming and Data Handling Foundations\n",
        "Week 3 - 4: Machine Learning Foundations; Supervised Learning\n",
        "Week 5 - 6: Model Evaluation and Tuning; Unsupervised Learning\n",
        "\n",
        "Now let's start learning about Deep Learning!"
      ],
      "metadata": {
        "id": "grMowX2U0ZfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks and Deep Learning\n",
        "\n",
        "Deep learning is a subset of machine learning that aims more closely mimic human thought. In order to do this, models called neural networks are employed; they are literally built like the neurons in a brain.\n",
        "\n",
        "\n",
        "\n",
        "Each neuron performs some sort of computation, and by interconnecting many neurons a neuron network is created.\n",
        "\n",
        "You can think of a neuron like an equation, let's say for instance: y = wx + b. You are already familiar with machine learning with linear equations when doing linear regression, but lets take a look at an example anyways; most notably, instead of scikit-learn we will be using the tensorflow and keras libraries.\n"
      ],
      "metadata": {
        "id": "aET5w4p7ojxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#Linear relationship: 2x + 1\n",
        "x = np.array([0, 1, 2, 3, 4], dtype=float)\n",
        "y = np.array([1, 3, 5, 7, 9], dtype=float)\n",
        "\n",
        "#This (keras.Sequential) creates a neural network as a stack of layers\n",
        "#In this particular case, this is only one neuron\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1, input_shape=[1])#number of units=number of outputs,input shape = number of feature inputs\n",
        "])\n",
        "\n",
        "#I didn't explicitly teach you about optimizers, but here I am using gradient descent to reduce loss\n",
        "#Loss is just how far off the predictions are from the values\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yh1zJQRqy3N",
        "outputId": "70b36814-faa9-4e6d-d2f7-6d9bb0539f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#An epoch is a pass through the training set.\n",
        "#1 epoch: Forward pass through training set, weight adjustment, backward pass through training set (backpropagation)\n",
        "model.fit(x, y, epochs=500, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbE56xfdsjaR",
        "outputId": "ecfdda12-7666-4d74-f96b-3eb4fa3540d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a2c97c36510>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict(np.array([10.0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39fsir5ePetI",
        "outputId": "f6a794e0-0895-4ebf-e1da-ef57dc37e693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "[[21.000628]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I'll go through a few things in this little program, and other related concepts that I think you need to know. Some these I have already mentioned, or suggested further learning that explain them, but just in case I'll go through them here.\n",
        "\n",
        "## Cost Functions\n",
        "Cost functions calculates the cost/error/loss, ie or the number of mistakes that the model makes. We can use the error information to improve the model.\n",
        "\n",
        "Examples of cost functions:\n",
        "\n",
        "*   Mean Squared Error\n",
        "*   Root Mean Squared Error\n",
        "*   Mean Absolute Error\n",
        "*   Crossentropy\n",
        "\n",
        "Once we have evaluated the cost of the model using a cost function, or next step is to minimize cost, which can be done using gradient descent.\n",
        "\n",
        "## Backpropogation\n",
        "\n",
        "Backpropogation happens after a forward pass through the training set, and conputes the gradient of the loss functions. These gradients are used during gradient descent.\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "Gradient descent is used to optimize the cost function to reduce loss (for the math geeks, usually it differentiates the cost function to produce the smallest possible loss, using OLS).\n",
        "\n",
        "It is most important to note that gradient descent will adjust the weight of a particular neuron, with the weight of a neuron being how much influence it has.\n",
        "\n",
        "In the above example I am doing stochastic gradient descent (SGD), which updates the weight sof the neurons after each training example. Other types of gradient descent are batch gradient descent, and mini-batch gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZIhmJPRfwxQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron\n",
        "\n",
        "A neural network is built with many neurons. To be specific, it is made up of layers. Generally, they consist of the following:\n",
        "\n",
        "*   Input Layer, with a neuron for each input\n",
        "*   Hidden layers, for example dense layers\n",
        "*   Output layer, with a neuron for each possible prediction for the model\n",
        "\n",
        "A multilayer perceptron is simply a neural network of many layers.\n",
        "\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "An activation function determins the output of a neuron based on a particular input. It can introduce non-linearity while transofrming inputs to outputs, so that a neural network can learn more complicated relationships. Think of it as what causes a neuron to fire like a brain neuron would. Here are a few examples\n",
        "*   ReLU\n",
        "*   Sigmoid\n",
        "*   Tanh\n",
        "\n",
        "## Overfitting vs Underfitting\n",
        "Underfitting occurs when the model doesn't learn enough.\n",
        "\n",
        "Overfitting occurrs when instead of learning the relationship between the inputs and outputs, a model instead learns the answers to the training data. There are quite a few ways to deal with overfitting, such as\n",
        "*    Early stopping: Stopping training early if the model stops improving\n",
        "*    Dropout: Randomly get rid of some neurons during the training process\n",
        "*    Batch normalizatioon: Normalize the input layer"
      ],
      "metadata": {
        "id": "Sl5C4TpVQzFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity\n",
        "\n",
        "Today, I would like you to build a neural network.\n",
        "\n",
        "I would like you to use [this](https://www.digitalocean.com/community/tutorials/constructing-neural-networks-from-scratch) source to create it.\n",
        "\n"
      ],
      "metadata": {
        "id": "ibKiocrAqJsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework\n",
        "\n",
        "Learn more about how deep learning and neural networks are used in computer vision and natural language processing.\n",
        "\n",
        "*   [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\n",
        "\n",
        "*   [Word Embedding](https://towardsdatascience.com/implementing-word2vec-in-pytorch-from-the-ground-up-c7fe5bf99889/?gi=fab8ad64eb2a)\n",
        "\n",
        "*   [Transformers](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
        "\n",
        "\n",
        "*   [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)"
      ],
      "metadata": {
        "id": "JPkcL1DQtK6Y"
      }
    }
  ]
}